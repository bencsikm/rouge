#pragma using math
#DEFINE THIS_IS_A_CONSTANT 1

FUNCTION GLOBAL Statistics::strings
    VAR
        stringArray : ARRAY[0..29] OF CHAR;
    END_VAR
    
    _memset(#stringArray[0], 0, 30);
    _strcpy(#stringArray[0], "This is a literal string");

    stringArray[0] = 'a';
    stringArray[0] = 'b';
    stringArray[0] = 'c';

END_FUNCTION


(*
    Gradient
    
    Same idea as MSE in that it was written for univariate. Can be updated for multivariate if needed.
    Calculates the new gradient values or thetas using the partial derivative of the MSE function and applying a learning rate.
    
    theta0 = theta0 - alpha * ( d/d*theta0 J(theta0, theta1) )
    
    where J(theta0, theta1) is the MSE function
    
    Original theta value minus the learning rate times the partial derivative of the MSE function with respect to the specific theta.
    This is done for every theta value. 
    
    Parameters:
        xBuffer 	: ^REAL;    Pointer to array of input variables (independent variable)
        yBuffer 	: ^REAL;    Pointer to array of output variables (dependent variable)
        thetaBuffer : ^REAL;    Pointer to array of theta values (Requires 2 values). These values will be updated as the output of the method.
        bufferSize 	: DINT;     Number of samples
		alpha 	    : REAL;     Learning rate. Start with something such as 0.1 and increase/decrease as needed by a multiple of 3
                                Round to keep simple. i.e. 10, 3, 1, 0.3, 0.1, 0.03, 0.01
    Ouputs:
        OutParam    : DINT;     0 indicates no error. Less than 0 indicates an error. See defines.
                                
*)
FUNCTION GLOBAL Statistics::gradient
	VAR_INPUT
		xBuffer 	: ^REAL;
		yBuffer 	: ^REAL;
		thetaBuffer : ^REAL;
		bufferSize 	: DINT;
		alpha 	    : REAL;
	END_VAR
	VAR_OUTPUT
		OutParam 	: DINT;
	END_VAR
    VAR
        theta0      : REAL;
        theta1      : REAL;
        sum0        : LREAL;
        sum1        : LREAL;
        i           : DINT;
        xVal        : REAL;
        yVal        : REAL;
    END_VAR

    if xBuffer = NIL | yBuffer = NIL | thetaBuffer = NIL then
        OutParam := ERROR_INVALID_POINTER_REFERENCE;
        return;
    elsif bufferSize < MIN_BUFFER_SIZE then
        OutParam := ERROR_INVALID_BUFFER_SIZE;
        return;
    else
        OutParam := ERROR_NONE;
    end_if;
    
    // get theta values
    theta0 := thetaBuffer^;
    theta1 := (thetaBuffer + sizeof(REAL))^;
    
    sum0 := 0.0;
    sum1 := 0.0;
    
    // summation
    FOR i := 0 TO bufferSize - 1 DO
    
        xVal := (xBuffer + i*sizeof(REAL))^;
        YVal := (yBuffer + i*sizeof(REAL))^;

        sum0 += (theta0 + (theta1 * xVal) - yVal);
        sum1 += (theta0 + (theta1 * xVal) - yVal) * xVal;
        
    END_FOR;
    
    // divide by sample size
    thetaBuffer^                   := theta0 - alpha * to_real(sum0 / to_lreal(bufferSize));
    (thetaBuffer + sizeof(REAL))^  := theta1 - alpha * to_real(sum1 / to_lreal(bufferSize));

END_FUNCTION

(*
    Gradient Descent - A form of linear regression
    
    Once complete, the resulting h(x) will be a best fit line calculated from the discrete data, y(x)
        
    Continually calculate new gradients using the learning rate.
    Checks the cost (error) of the newly calculated gradient values.
    The cost should continually drecrease until the cost (error) converges. 
    
    Convergence is equated to an incrementally small change in the cost after new gradients are calculated.
    
    Parameters:
        xBuffer 	: ^REAL;    Pointer to array of input variables (independent variable)
        yBuffer 	: ^REAL;    Pointer to array of output variables (dependent variable)
        theta 	    : ^REAL;    Pointer to array of theta values. These values will be updated as the output of the method.
        bufferSize 	: DINT;     Number of samples
		alpha 	    : REAL;     Learning rate. Start with something such as 0.1 and increase/decrease as needed by a multiple of 3
                                Round to keep simple. i.e. 10, 3, 1, 0.3, 0.1, 0.03, 0.01
    Ouputs:
        cost 	    : REAL;     MSE. If 0, then there is no error between your predicted value and actual value. 

*)
FUNCTION GLOBAL Statistics::gradientDescent
	VAR_INPUT
		xBuffer 	: ^REAL;
		yBuffer 	: ^REAL;
		thetaBuffer : ^REAL;
		bufferSize 	: DINT;
		alpha 	    : REAL;
	END_VAR
	VAR_OUTPUT
		cost 	    : REAL;
	END_VAR
    VAR
    	i           : DINT;
        lastCost    : REAL;
        errorCheck  : DINT;
    END_VAR
    
    // Calculate an initial cost to determine that the gradient is descending
    lastCost := meanSquaredError(xBuffer, yBuffer, thetaBuffer, bufferSize);
    
    if lastCost < ERROR_NONE then
        cost := lastCost;
        return;
    end_if;

    // gradient descent process 
    FOR i := 1 TO MAX_DESCENT_ITERATION_COUNT - 1 DO
        
        // calculate new thetas using learning rate
        errorCheck := gradient(xBuffer, yBuffer, thetaBuffer, bufferSize, alpha);
        if errorCheck < ERROR_NONE then
            cost := errorCheck;
            exit;
        end_if;

        // get new cost (error) value associated with the new thetas
        cost := meanSquaredError(xBuffer, yBuffer, thetaBuffer, bufferSize);

        // check for convergence
        if cost < ERROR_NONE | (lastCost > cost & lastCost - cost < CONVERGENCE_VALUE) then
            exit;
        end_if;
        
        lastCost := cost;
        
    END_FOR;
    
    if i >= MAX_DESCENT_ITERATION_COUNT then
        cost := ERROR_FAILED_TO_CONVERGE;
    end_if;
    
END_FUNCTION

(*
    Mean
    
    Calculates the mean of a buffer of values
    
    Parameters:
        buffer 	    : ^REAL;    buffer of values
		bufferSize 	: DINT;     size of the buffer
    
    Output:
        mu 	        : REAL;         mean of the buffer
    
*)
FUNCTION GLOBAL Statistics::mean
	VAR_INPUT
		buffer 	    : ^REAL;
		bufferSize 	: DINT;
	END_VAR
	VAR_OUTPUT
		mu 	        : REAL;
	END_VAR
    VAR
    	i           : DINT;
        sum         : REAL;
    END_VAR
    
    if buffer = NIL then
        mu := ERROR_INVALID_POINTER_REFERENCE;
        return;
    elsif bufferSize < MIN_BUFFER_SIZE then
        mu := ERROR_INVALID_BUFFER_SIZE;
        return;
    end_if;
    
    sum := 0.0;
    
    //mu = sum(buffer) / size
    for i := 0 to bufferSize - 1 do
        sum += (buffer + i * sizeof(REAL))^;
    end_for;
    
    mu := sum / to_real(bufferSize);
    
END_FUNCTION

(*
    Mean Squared Error - Cost Function (MSE)

    Originally written as a univariate function (assumes 1 feature)
    To update to a multivariate function, add the number of features as an input.
    Then add a theta for each additional feature, and an X buffer for each feature set.
    theta0 is always present. theta1 for 1st feature, theta2 for 2nd feature... and so on
    
    Cost Function:
    
        J(theta0, theta1) = (1 / 2m) * sum( (h(x) - y(x))^2 )
        
        The larger the value, the greater the error between your predicted and actual values
        
        Where,
            x       -> input or independent variable,   i.e. time
            y(x)    -> output or dependent variable,    i.e. melt pressure
            h(x)    -> predicted values,                i.e. best fit line
                        h(x) = theta0 + theta1 * x      (straight line)
            m       -> buffer size
            
    Parameters:
        xBuffer 	: ^REAL;    Pointer to array of input variables (independent variable)
        yBuffer 	: ^REAL;    Pointer to array of output variables (dependent variable)
        thetaBuffer : ^REAL;    Pointer to array of theta values (Must be 2 values at the moment)
        bufferSize 	: DINT;     Number of samples
       
    Ouputs:
        OutParam    : REAL;     MSE. If 0, then there is no error between your predicted value and actual value.  
        
*)
FUNCTION GLOBAL Statistics::meanSquaredError
	VAR_INPUT
		xBuffer 	: ^REAL;
		yBuffer 	: ^REAL;
		thetaBuffer : ^REAL;
		bufferSize 	: DINT;
	END_VAR
	VAR_OUTPUT
		cost 	    : REAL;
	END_VAR
    VAR
    	sum         : LREAL;
        i           : DINT;
        theta0      : REAL;
        theta1      : REAL;
        xVal        : REAL;
        yVal        : REAL;
    END_VAR
    
    if xBuffer = NIL | yBuffer = NIL | thetaBuffer = NIL then
        cost := ERROR_INVALID_POINTER_REFERENCE;
        return;
    elsif bufferSize < MIN_BUFFER_SIZE then
        cost := ERROR_INVALID_BUFFER_SIZE;
        return;
    end_if;
    
    // get theta values
    theta0 := thetaBuffer^;
    theta1 := (thetaBuffer + sizeof(REAL))^;
    
    sum := 0.0;
    
    // Sum error squared
    FOR i := 0 TO bufferSize - 1 DO
    
        xVal := (xBuffer + i*sizeof(REAL))^;
        yVal := (yBuffer + i*sizeof(REAL))^;
        sum += to_lreal((theta0 + (theta1 * xVal) - yVal))**2;
        
    END_FOR;
    
    // divide by 2 * num samples
    cost := to_real(sum / to_lreal(2 * bufferSize));

END_FUNCTION

(*
    Standard Deviation
    
    Calculates the standard deviation of a buffer of values
    
    Uncorrected (Population)
    sigma = sqrt ( 1/(N) SUM_i (X(i) - mean(X))^2 )
    
    Corrected (Sample)
    sigma = sqrt ( 1/(N-1) SUM_i (X(i) - mean(X))^2 )
    
    Parameters:
        buffer 	    : ^REAL;    buffer of values
		bufferSize 	: DINT;     size of the buffer
        mu 	        : REAL;     mean of the buffer
        corrected 	: BOOL;     sample or corrected std dev
                                TRUE    = (N-1)
                                FALSE   = (N)
        
    Output:
        sigma 	    : REAL;     the standard deviation
    
*)
FUNCTION GLOBAL Statistics::standardDeviation
	VAR_INPUT
		buffer 	    : ^REAL;
		bufferSize 	: DINT;
		mu 	        : REAL;
		corrected 	: BOOL;
	END_VAR
	VAR_OUTPUT
		sigma 	    : REAL;
	END_VAR
    VAR
        sum         : REAL;
        i           : DINT;
    END_VAR
    
    if buffer = NIL then
        sigma := ERROR_INVALID_POINTER_REFERENCE;
        return;
    elsif corrected & bufferSize < MIN_BUFFER_SIZE_STD_DEV_SAMPLE then
        sigma := ERROR_INVALID_BUFFER_SIZE;
        return;
    elsif !!corrected & bufferSize < MIN_BUFFER_SIZE then
        sigma := ERROR_INVALID_BUFFER_SIZE;
        return;
    end_if;
    
    sum := 0.0;
    
    for i := 0 to bufferSize - 1 do
        sum += ((buffer + i * sizeof(REAL))^ - mu)**2;
    end_for;
    
    if corrected then
        bufferSize -= 1;
    end_if;

    sigma := sqrt((1.0 / to_real(bufferSize)) * sum);

END_FUNCTION

(*
    Standardization
    
    Normalizes a buffer using the mean and std deviation.
    Used to scale multiple buffers to the same scale based on the data in the buffer.
    Resulting values will be in standard deviations from the mean of the input data.
    
    z = (x - mu) / sigma
    
    where   z -> nomalized value
            x -> input value
            mu -> mean
            sigma -> std dev
              
    Parameters:
        buffer 	    : ^REAL;    Values to be scaled. values are modified.
		bufferSize 	: DINT;     Size of the buffer
		mu 	        : ^REAL;    output value of the mean
		sigma 	    : ^REAL;    output value of the std dev.
*)
FUNCTION GLOBAL Statistics::standardization
	VAR_INPUT
		buffer 	    : ^REAL;
		bufferSize 	: DINT;
		mu 	        : ^REAL;
		sigma 	    : ^REAL;
	END_VAR
	VAR_OUTPUT
		OutParam 	: DINT;
	END_VAR
    VAR
        i           : DINT;
    END_VAR
    
    if buffer = NIL | mu = NIL | sigma = NIL then
        OutParam := ERROR_INVALID_POINTER_REFERENCE;
        return;
    else
        OutParam := ERROR_NONE;
    end_if;

    mu^     := mean(buffer, bufferSize);
    sigma^  := standardDeviation(buffer, bufferSize, mu^, FALSE);
    
    // if the std dev is 0, then all values in buffer are identical.
    // This would yield 0/0 for all values. 
    if sigma^ = 0.0 then    
        OutParam := ERROR_STANDARDIZATION_FAIL;
        return;
    end_if;

    for i := 0 to bufferSize - 1 do
        (buffer + i * sizeof(REAL))^ := ((buffer + i * sizeof(REAL))^ - mu^) / sigma^;
    end_for;

END_FUNCTION